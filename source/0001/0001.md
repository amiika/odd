# How to FZRTH

In this tutorial, we will build a minimalistic live coding environment, starting from an empty HTML boilerplate and ending up with a novel live audio programming language interpreted and running entirely in the browser's high priority audio thread.

## Part 1: The Architecture & Timing

The heart of any live coding language is the timing. There are a lot of different ways to time things in the web. [Some](https://loophole-letters.vercel.app/web-audio-scheduling) better than others. 
However, they all have the same flaw: The Browser's main thread gets too busy and can't keep up. 
No matter what you cannot completely escape the firm grip of the main thread - or could you?

The isolation is critical. As explained in the [Web Audio Spesification](https://www.w3.org/TR/webaudio-1.1/#AudioWorklet-concepts), the **main thread** handles the DOM and application level interaction, while the **high priority audio thread** can handle the audio rendering. Communication between them happens via asynchronous messaging. Due to the main thread getting too busy, this architecture can still lead to delayed messages and glitches in the garbage collection. To avoid this issue, the application architecture must be optimized to have less impact on the main thread and schedule ahead whenever possible.

## Part 4: Implementation

This tutorial series is about exploring the esoteric and unthinkable. In this case, avoid using the main thread for interpreting and scheduling while synthetizing all sound using a custom WebAudioProcessor. For this first part of the tutorial we will implement the bare bones of the interpreter and the audio processor as a proof of concept.

The idea is that when you type in the live coding editor, we will send all changes to the interpreter using asynchronous messaging. When the the processor receives the updated code, it interprets it and continues rendering and scheduling the audio without interruptions. This means that even if the main thread freezes due to heavy UI rendering, the audio should never glitch or stutter (at least not because of the main thread getting busy).

### FZRTH - The language

We are going to implement a live coding language based on Forth (The programming language that writes itself), inspired by this great [presentation](https://ratfactor.com/forth/the_programming_language_that_writes_itself.html).

As stated: *"To understand Forth, you have to implement a Forth" -- Somebody on the Internet*

However, we are not going to be so strict on the details. We will stray to uncharted lands and explore the world of [uzulangs](https://uzu.lurk.org/). We will try to stay true to the core ideas of the Forth language while also including some of the [mininotation](https://tidalcycles.org/docs/reference/mini_notation/) proven to work with live coding. Not all by any means, but just enough to make things interesting.

To really understand and learn Forth I recommend reading the free [Starting Forth](https://www.forth.com/starting-forth/) book. Arguably one of the greatest programming books ever made. It's fun and easy, and if you are not familiar with stack based programming languages quick skim will explain a lot more than this tutorial will do. We will only create a minimal interpreter for Forth-like postfix syntax to JavaScript, which is already a complete abomination ... so if you are looking for a tutorial on how to write a FORTH compiler, this tutorial is not for you.

### Interpreter

Our Forth-like interpreter is just a lightweight JavaScript parser running inside the browser's high priority audio thread. It takes the raw text from the editor, splits it into tokens, and executes commands found from the dictionary against the stack on the fly.

The Dictionary (`DICT`) maps all known words to functions. When the tokenizer encounters a number, it pushes it to the stack. When it encounters a known word (like `sine`), it executes the function, consuming the stack to produce audio objects which are pushed into the `newSounds` array.

```javascript
const DICT = {
    'sine': (stack, sounds) => {
        if (stack.length > 0) {
            // Consumes the stack to create a new voice
            sounds.push({
                freqs: [...stack], // Copy stack frequencies
                phase: 0           // Initialize phase for continuity
            });
            stack.length = 0;      // Clear the stack
        }
    }
};
```

Inside the `compile` method, we initialize an empty `stack` to hold the data. We also initialize a **newSounds**-array for the audio objects we are about to create.

When creating these audio objects, we initialize a `phase` property. Compared to some other real live coding languages, this gimmick operates on a sample level, meaning it is also a digital signal processor (DSP). The phase tracks the oscillator's current position in its sine cycle. When live coding, you frequently re-execute the code. If a sound exists at index `i` in the previous frame, we copy its `phase` to the new sound to ensure seamless transitions.


```javascript
// Inside the Processor class:
compile(text) {
    // 1. Initialization: Create fresh containers
    const tokens = text.split(/\s+/).filter(x => x); 
    const stack = [];
    const newSounds = []; 

    // 2. Interpreter Loop
    tokens.forEach(token => {
        const num = parseFloat(token);
        
        if (!isNaN(num)) {
            // Push numbers to stack
            stack.push(num); 
        } else if (DICT[token]) {
            // Execute words from Dictionary
            // We pass 'newSounds' so the word can push created voices into it
            DICT[token](stack, newSounds); 
        }
    });
    
    // 3. Phase Preservation logic
    // We loop through the new sounds and see if there was a sound
    // at the same index in the previous 'this.sounds' array.
    newSounds.forEach((sound, index) => {
        if (this.sounds[index]) {
            // If it existed, we carry over the phase state.
            // This prevents the waveform from resetting to 0 (clicking).
            sound.phase = this.sounds[index].phase;
        }
    });
    
    this.sounds = newSounds;
}
```
### Processor

The `process(inputs, outputs)` method is the audio engine of our synth. It runs repeatedly on the audio thread to fill audio buffers.

* **outputs[0][0]** - The Output Buffer. This is a Float32Array representing the audio signal for the current rendering quantum. Audio is populated to this array with amplitude values between -1.0 and 1.0.
* **sampleRate** - The Sampling Rate. The number of data points processed per second (typically 44,100). It defines the resolution of our system.
* **currentFrame** - The Clock. Increasing integer representing exactly how many samples have passed since the engine started. It allows for precise, sample-accurate scheduling without floating point errors.

For now, to create a rhythmic loop, we apply the modulo operator (`%`) to the `currentFrame`. This transforms the infinite linear time into a cyclic beat time, counting from 0 to the end of a beat and then resetting instantly. We map the continuous time within the beat loop to a discrete index in the sequence array. This determines which frequency from the stack is active for the current sample.

To generate sound, we iterate through every sample index in the output buffer and calculate the instantaneous amplitude:

1.  **Phase Accumulation:** We increment the oscillator's phase. Think of this as a counter tracking how far we are through the current wave cycle. 0.0 is the start, 0.5 is halfway, and 1.0 means a full cycle is completed. Higher frequency adds bigger steps, completing cycles faster.
2.  **Wave Shaping:** We convert this linear progress (0 to 1) into an angle (radians) and feed it to `Math.sin()`. This creates the smooth up-and-down oscillation of the sound wave.
3.  **Summation:** We combine the signals from all active voices and apply gain to ensure the total amplitude remains within the accepted range.



```javascript
process(inputs, outputs) {
    const output = outputs[0][0]; // Mono Left channel
    const samplesPerBeat = sampleRate * 0.5; // 120 BPM

    for (let i = 0; i < output.length; i++) {
        // 1. Timing: Calculate position in the beat
        const globalTime = currentFrame + i;
        const sequenceTime = globalTime % samplesPerBeat;
        
        let signal = 0;

        // 2. DSP: Synthesize every voice
        this.sounds.forEach(voice => {
            if (voice.freqs.length === 0) return;
            
            // Determine which note in the stack to play
            const step = samplesPerBeat / voice.freqs.length;
            const noteIndex = Math.floor(sequenceTime / step);
            const freq = voice.freqs[noteIndex] || 0;
            
            // Advance phase based on frequency
            voice.phase += (freq / sampleRate);
            
            // Generate Sine Wave (0 to 1 phase mapped to 0 to 2PI)
            signal += Math.sin(voice.phase * 2 * Math.PI);      
        });

        // 3. Output: Write to buffer with gain reduction
        output[i] = signal * 0.2;
    }
    return true;
}
```

## Live Embedded Demo

This implementation is just a start on our journey. Below is the first functional embedded version of our new live coding language and the environment. It is less than 100 lines of code, containing the parser, dictionary, sequencer and synthetizer.

**INSTRUCTIONS:** Click the box below to boot the audio engine. Then edit the numbers to change the melody. Try adding `800 900 sine` at the end of the line to play two lines simultaneously. 

This is also a good way to make those around you uncomfortable.

**NOTE**: FZRTH is live evaluated language. To **STOP** just remove the code (or the playing operator `sine`) from the input.

```fzrth
100 200 300 400 sine
```

[Open full Version](./source/0001/0001.html)

Next we will add a BPM operator and more features from the FORTH language.

[Back to Index](../index.html)


<details>
<summary>Full source</summary>


HTML:
```html
<!DOCTYPE html>
<html>
<head>
    <title>FZRTH</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        body { background: #000; color: #fb0; font-family: monospace; display: flex; flex-direction: column; height: 100vh; margin: 0; overflow: hidden; }
        #editor { flex: 1; background: #000; color: #fb0; border: none; padding: 20px; font-size: 20px; outline: none; resize: none; width: 100%; box-sizing: border-box; }
        .header { font-size: 24px; padding: 15px; border-bottom: 1px solid #fb0; display: flex; justify-content: space-between; align-items: center; background: #111;}
        ::selection { background: #fb0; color: #000; }
    </style>
</head>
<body>
    <div class="header">
        <div>FZRTH</div>
    </div>
    <textarea id="editor" spellcheck="false">100 200 300 400 sine</textarea>
    <script>
        const editor = document.getElementById('editor');
        let audioCtx;
        let node;
        async function init() {
            if (audioCtx) return;
            try {
                audioCtx = new (window.AudioContext || window.webkitAudioContext)();
                await audioCtx.audioWorklet.addModule('0001.js');
                node = new AudioWorkletNode(audioCtx, 'fzrth-proc');
                node.connect(audioCtx.destination);
                editor.addEventListener('input', () => node.port.postMessage(editor.value));
                node.port.postMessage(editor.value);         
            } catch (e) {
                console.error("AudioWorklet Error:", e);
            }
        }
        document.addEventListener('click', init, { once: true });
    </script>
</body>
</html>
```


AudioWorkletProcessor code:

```javascript
const DICT = {
    'sine': (stack, sounds) => {
        if (stack.length > 0) {
            sounds.push({
                freqs: [...stack], 
                phase: 0
            });
            stack.length = 0;
        }
    }
};

class ForthProcessor extends AudioWorkletProcessor {
    constructor() { 
        super(); 
        this.sounds = []; 
        this.port.onmessage = (e) => this.interpret(e.data); 
    }
    
    interpret(text) {
        const tokens = text.split(/\s+/).filter(x => x);
        const stack = [];
        const newSounds = [];
        
        tokens.forEach(token => {
            const num = parseFloat(token);
            if (!isNaN(num)) {
                stack.push(num);
            } else if (DICT[token]) {
                DICT[token](stack, newSounds);
            }
        });
        
        newSounds.forEach((sound, index) => {
            if (this.sounds[index]) {
                sound.phase = this.sounds[index].phase;
            }
        });
        
        this.sounds = newSounds;
    }

    process(inputs, outputs) {
        const output = outputs[0][0];
        if (!output) return true;
        
        const samplesPerBeat = sampleRate * 0.5; // 120 BPM

        for (let i = 0; i < output.length; i++) {
            const globalTime = currentFrame + i;
            const sequenceTime = globalTime % samplesPerBeat;
            let signal = 0;

            this.sounds.forEach(voice => {
                if (voice.freqs.length === 0) return;
                
                const step = samplesPerBeat / voice.freqs.length;
                const noteIndex = Math.floor(sequenceTime / step);
                const freq = voice.freqs[noteIndex] || 0;
                
                voice.phase += (freq / sampleRate);
                signal += Math.sin(voice.phase * 2 * Math.PI);      
            });

            output[i] = signal * 0.2;
        }
        return true;
    }
}
registerProcessor('fzrth-proc', ForthProcessor);
```

</details>