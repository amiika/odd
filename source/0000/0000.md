# Isometric Frequency Maps

This prototype explores the concept of **Isometric Pitch Spaces**, which maps frequencies to space in a way that notes, chords or scales form identical shapes. The logic for creating such isometric pitch spaces can vary, and in this case we are exploring simple logarithmic pitch space that maps frequencies diagonally. The result will be simple playable instrument where "Up and Right" equals higher pitch, and "Down and Left" equals lower pitch, creating a continuous field of sound ranging from 20Hz to 20kHz.

## 1. The Isometric Frequency Math

The core challenge is mapping linear screen coordinates to human pitch perception, which is logarithmic. We use the **Weber-Fechner Law**, essentially calculating "octaves" by doubling the frequency.

We distribute the total available octaves across the screen's aspect ratio to ensure the "grid" of notes feels square and uniform regardless of screen size.

```javascript
const getSpatialFreq = (x, y) => {
    // 1. Calculate total pitch range in Octaves (approx 10 for 20Hz-20kHz)
    const totalOctaves = Math.log2(MAX_FREQ / MIN_FREQ); 
    const ratio = width / height;
    
    // 2. Distribute octaves based on aspect ratio
    const octavesY = totalOctaves / (ratio + 1);
    const octavesX = totalOctaves - octavesY;

    // 3. Normalize coordinates (0.0 to 1.0)
    // Note: We invert Y so the bottom of the screen is "low"
    const normX = x / width;
    const normY = (height - y) / height;

    // 4. Calculate the diagonal offset
    const octaveOffset = (octavesX * normX) + (octavesY * normY);
    
    // 5. Convert back to Frequency: f = base * 2^octaves
    return MIN_FREQ * Math.pow(2, octaveOffset);
};
```

## 2. The Audio Engine

The Audio Engine for our synth uses default gain node to control the amplitude and **AudioWorklet** to produce the signal. The main thread handles the user interface and drawing while the Worklet runs on a high-priority background thread.

### Phase Accumulation
We generate the waveform sample-by-sample. We track a `phase` variable that moves from 0.0 to 1.0. The speed at which it moves determines the pitch.

```javascript
// Inside the Worklet Processor loop
for (let i = 0; i < output.length; i++) {
    // Get frequency for this specific sample 
    const f = isFArr ? freqs[i] : freqs[0];

    // Increment and wrap phase
    this.phase += f / sampleRate;
    if (this.phase > 1.0) this.phase -= 1.0;
    
    // Output Sine: map 0.0-1.0 to 0-2PI
    output[i] = Math.sin(2.0 * Math.PI * this.phase);
}
```

## 3. The Visual Feedback

To create the oscilloscope aesthetic, we do not clear the canvas every frame. Instead, we paint a semi-transparent black rectangle over it. This causes old lines to slowly fade out, creating a persistence-of-vision trail effect.

```javascript
const animate = () => {
    // Draw 10% opaque black over the screen
    ctx.fillStyle = 'rgba(0, 0, 0, 0.1)';
    ctx.fillRect(0, 0, width, height);
    
    // Draw new lines on top...
    requestAnimationFrame(animate);
};
```

## 4. Communication: Main Thread to Worklet

The browser enforces a strict separation between the Main Thread (UI) and the Audio Thread. We cannot simply set a variable inside the synth from a mouse event. We must use the **AudioParam** system.

### The Flow of Data
1.  **User Input:** The user moves the mouse. The Main Thread captures `clientX` and `clientY`.
2.  **Calculation:** `getSpatialFreq(x, y)` converts pixels to Hertz.
3.  **Scheduling:** We use `synthNode.parameters.get('freq').setTargetAtTime(...)` to schedule the change.

This ensures that even if the visual frame rate drops, the audio pitch changes smoothly without clicking or popping.

# Interaction & The Event Loop

While the Audio Worklet handles the heavy lifting of signal processing, the **Main Thread** is responsible for capturing human intent. This section of the code manages the bridge between physical gesture (mouse/touch) and system response (sound/light).

We break the interaction down into three distinct phases common to all musical instruments: **Attack** (Start), **Sustain** (Move), and **Release** (End).

## 1. The Attack Phase

This function triggers immediately when the user touches the screen or clicks the mouse. It is responsible for waking up the synth and initiating the sound.

```javascript
const start = (x, y) => {
    if (!audioCtx) return;
    isPlaying = true;
    lx = x; 
    ly = y;
    
    // 1. Pitch Initialization
    // We update the audio immediately (0.001s) so the note starts 
    // at the correct pitch, rather than sliding from a previous value.
    updateAudio(lx, ly, 0.001); 
    
    // 2. The Volume Envelope (Attack)
    // We do not set gain to 0.5 instantly. That would cause a "pop" or "click".
    // Instead, we use setTargetAtTime to smoothly ramp the volume up 
    // over 20ms (0.02s), creating a natural, soft onset.
    gainNode.gain.setTargetAtTime(0.5, audioCtx.currentTime, 0.02);
};

const updateAudio = (x, y, timeConstant = 0.05) => {
    if (!synthNode) return;
    const t = audioCtx.currentTime;
    const freq = getSpatialFreq(x, y);
    // Prevent clicks
    synthNode.parameters.get('freq').setTargetAtTime(freq, audioCtx.currentTime, timeConstant);
};
```

## 2. The Sustain Phase

This function runs continuously (up to 60+ times a second) while the user drags their cursor. It handles the continuous modulation of pitch and the rendering of the visual trail.

```javascript
const move = (x, y) => {
    if (!isPlaying) return;
    
    // VISUAL TRAIL

    ctx.beginPath();
    ctx.moveTo(lx, ly);
    ctx.lineTo(x, y);
    ctx.strokeStyle = '#0f0';
    ctx.lineWidth = 3; 
    ctx.shadowBlur = 15;
    ctx.shadowColor = '#0f0';
    ctx.lineCap = 'round';
    ctx.lineJoin = 'round';
    ctx.stroke();
    ctx.shadowBlur = 0;

    // Update state
    lx = x;
    ly = y;

    // Send new coordinates to the Audio Engine
    updateAudio(lx, ly);
};
```

## 3. The Release Phase

This triggers when the user lifts their finger. Just like the attack, we cannot simply cut the audio, or it will sound mechanical and "clicky."

```javascript
const end = () => {
    if (!gainNode) return;
    isPlaying = false;
    // Ramp the gain down to 0 over 50ms (0.05s).
    gainNode.gain.setTargetAtTime(0, audioCtx.currentTime, 0.05);
};
```

## 4. Testing the space

```pitchspace
```

## 5. Alternative ways

Creating custom synth using WebAudio Processor is only one way to sonify the pitch space. Alternatively different existing synth engines can be used like the supersonic or dough. For supercollider / scsynth based implementation see [supersonic example](./source/0000/supersonic.html).

## 5. Go nuts

This is just one example of isometric pitch space. Explore more spaces and possibilities in [synthoscope.com](https://synthoscope.com). It's and ongoing research project on scales, pitch spaces, audio synthesis and my personal playground for live coding and web audio development.


[Open full Version](./source/0000/0000.html)

In the next tutorial sections we start to figure out how to craft interpreter for a live coding language that lives entirely in the web audio thread. 

[Back to Index](../index.html)


<details>
<summary>Full source</summary>


HTML:
```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Isomorphic Pitch Space</title>
    <style>
        body { 
            margin: 0; 
            background: #000; 
            overflow: hidden; 
            cursor: crosshair; 
            font-family: monospace; 
            color: #0f0;
            user-select: none;
            touch-action: none;
        }
        canvas { display: block; width: 100vw; height: 100vh; }
        #overlay {
            position: absolute; top: 0; left: 0; width: 100%; height: 100%;
            display: flex; flex-direction: column;
            align-items: center; justify-content: center;
            background: rgba(0,0,0,0.8); z-index: 10;
            cursor: pointer;
        }
        #info {
            position: absolute; top: 10px; left: 10px;
            pointer-events: none;
            font-size: 16px;
            background: rgba(0, 0, 0, 0.5);
            padding: 5px;
        }
        h1 { margin: 0 0 10px 0; text-shadow: 0 0 10px #0f0; }
        p { color: #888; }
    </style>
</head>
<body>
    <div id="overlay">
        <h1>CLICK TO START</h1>
        <p>Isomorphic Pitch Space</p>
    </div>
    <div id="info">0 Hz</div>
    <canvas id="canvas"></canvas>
    <script>
        // DOM Elements
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        const overlay = document.getElementById('overlay');
        const info = document.getElementById('info');
        
        // State
        let width, height;
        let audioCtx, synthNode, gainNode;
        let isPlaying = false;
        let lx = 0, ly = 0; // Last x/y coordinates

        // Configuration
        const MIN_FREQ = 20;
        const MAX_FREQ = 20000;

        const init = async () => {
            if (audioCtx) {
                if (audioCtx.state === 'suspended') audioCtx.resume();
                overlay.style.display = 'none';
                return;
            }
            
            try {
                if (audioCtx) return;
                audioCtx = new (window.AudioContext || window.webkitAudioContext)();
                await audioCtx.audioWorklet.addModule('0000.js');
                synthNode = new AudioWorkletNode(audioCtx, 'sine-synth');
                gainNode = audioCtx.createGain();
                
                gainNode.gain.value = 0; // Start silent to prevent popping
                
                // Connect Graph: Synth -> Gain -> Speakers
                synthNode.connect(gainNode);
                gainNode.connect(audioCtx.destination);
                
                // Start Visual Loop
                animate();
                
            } catch (e) {
                console.error(e);
                alert("Failed to load AudioWorklet");
            }
        };

        // Resize Canvas to fill window
        const resize = () => {
            width = canvas.width = window.innerWidth;
            height = canvas.height = window.innerHeight;
        };
        window.addEventListener('resize', resize);
        resize();

        const getSpatialFreq = (x, y) => {
            // How many octaves fit in our range? (approx 10 octaves for 20-20k)
            const totalOctaves = Math.log2(MAX_FREQ / MIN_FREQ); 
            const ratio = width / height;
            
            // Split total octaves based on aspect ratio to keep "grid" square
            const octavesY = totalOctaves / (ratio + 1);
            const octavesX = totalOctaves - octavesY;

            // Normalize inputs (0.0 to 1.0)
            const normX = x / width;
            const normY = (height - y) / height; // Invert Y so bottom is 0

            // Calculate current octave offset
            const octaveOffset = (octavesX * normX) + (octavesY * normY);
            
            // Convert Octave back to Frequency: f = base * 2^octaves
            return MIN_FREQ * Math.pow(2, octaveOffset);
        };

        // UI Update: Show Hz on screen
        const updateFreqVisual = (x, y) => {
            const freq = getSpatialFreq(x, y);
            info.innerText = Math.round(freq) + " Hz";
        };

        // Audio Update: Send new frequency to the Worklet
        const updateAudio = (x, y, timeConstant = 0.05) => {
            if (!synthNode) return;
            const t = audioCtx.currentTime;
            
            const freq = getSpatialFreq(x, y);
            
            synthNode.parameters.get('freq').setTargetAtTime(freq, t, timeConstant);
        };

        overlay.addEventListener('click', init);

        const start = (x, y) => {
            if (!audioCtx) return;
            isPlaying = true;
            lx = x; 
            ly = y;
            
            // Immediate update for attack
            updateAudio(lx, ly, 0.001); 
            // Smoothly ramp up volume (Attack)
            gainNode.gain.setTargetAtTime(0.5, audioCtx.currentTime, 0.02);
        };

        const move = (x, y) => {
            if (!isPlaying) return;
            
            ctx.beginPath();
            ctx.moveTo(lx, ly);
            ctx.lineTo(x, y);
            
            ctx.strokeStyle = '#0f0'; // Green
            ctx.lineWidth = 3; 
            
            // Glow Settings
            ctx.shadowBlur = 15;
            ctx.shadowColor = '#0f0';
            
            ctx.lineCap = 'round';
            ctx.lineJoin = 'round';
            ctx.stroke();
            
            // Reset shadow to avoid performance hit on other operations
            ctx.shadowBlur = 0;

            lx = x;
            ly = y;
            updateAudio(lx, ly);
        };

        const end = () => {
            if (!gainNode) return;
            isPlaying = false;
            // Smoothly ramp down volume (Release)
            gainNode.gain.setTargetAtTime(0, audioCtx.currentTime, 0.05);
        };

        // --- Event Listeners ---
        
        // Mouse
        window.addEventListener('pointerdown', (e) => {
            updateFreqVisual(e.clientX, e.clientY);
            start(e.clientX, e.clientY);
        });
        window.addEventListener('pointermove', (e) => {
            updateFreqVisual(e.clientX, e.clientY);
            move(e.clientX, e.clientY);
        });
        window.addEventListener('pointerup', end);

        // Fade Trail Animation Loop
        const animate = () => {
            requestAnimationFrame(animate);
            ctx.fillStyle = 'rgba(0, 0, 0, 0.1)';
            ctx.fillRect(0, 0, width, height);
            ctx.shadowBlur = 0; 
        };

    </script>
</body>
</html>
```


AudioWorkletProcessor code:

```javascript
class SineSynth extends AudioWorkletProcessor {

    static get parameterDescriptors() {
        return [
            { 
                name: 'freq', 
                defaultValue: 440, 
                minValue: 20, 
                maxValue: 22000,
                automationRate: 'a-rate' // Explicitly 'a-rate' for smooth pitch bends
            }
        ];
    }

    constructor() {
        super();
        // Phase tracks the current position in the sine wave cycle (0.0 to 1.0).
        this.phase = 0;
    }

    /**
     * The Core DSP Loop
     * * @param {Float32Array[][]} inputs - Incoming audio (not used here).
     * @param {Float32Array[][]} outputs - Where we write our sound. [Output][Channel][Sample].
     * @param {Object} parameters - The current values of our defined parameters (freq).
     */
    process(inputs, outputs, parameters) {
        const output = outputs[0][0];
        if (!output) return true; 
        
        const freqs = parameters.freq;
        const isFArr = freqs.length > 1;

        for (let i = 0; i < output.length; i++) {
            const f = isFArr ? freqs[i] : freqs[0];
            this.phase += f / sampleRate;
            if (this.phase > 1.0) this.phase -= 1.0;
            output[i] = Math.sin(2.0 * Math.PI * this.phase);
        }

       return true;
    }
}

registerProcessor('sine-synth', SineSynth);
```

</details>